{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amankiitg/GenAI/blob/main/LLM_Scratch_Lecture_5_6_Byte_Pair_Encoding_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why does character level tokenization fail?"
      ],
      "metadata": {
        "id": "bHhrmYGJGYYI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzNb6UXDGVIw",
        "outputId": "649bb077-6b8c-4b48-fe33-4860bb18fcd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens: ['Today,', 'I', 'want', 'to', 'start', 'my', 'day', 'with', 'a', 'cup', 'of', 'coffee']\n",
            "Number of words: 12\n",
            "Character tokens: ['T', 'o', 'd', 'a', 'y', ',', ' ', 'I', ' ', 'w', 'a', 'n', 't', ' ', 't', 'o', ' ', 's', 't', 'a', 'r', 't', ' ', 'm', 'y', ' ', 'd', 'a', 'y', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'c', 'u', 'p', ' ', 'o', 'f', ' ', 'c', 'o', 'f', 'f', 'e', 'e']\n",
            "Number of characters: 50\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Today, I want to start my day with a cup of coffee\"\n",
        "\n",
        "# Split on whitespace to get individual words\n",
        "words = sentence.split()\n",
        "\n",
        "# Print the list of word tokens\n",
        "print(\"Word tokens:\", words)\n",
        "\n",
        "# Print the total number of words\n",
        "print(\"Number of words:\", len(words))\n",
        "\n",
        "\n",
        "sentence = \"Today, I want to start my day with a cup of coffee\"\n",
        "\n",
        "# Convert the sentence into a list of individual characters\n",
        "characters = list(sentence)\n",
        "\n",
        "# Print the list of character tokens\n",
        "print(\"Character tokens:\", characters)\n",
        "\n",
        "# Print the total number of characters\n",
        "print(\"Number of characters:\", len(characters))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "s = 'RRRRRRWs Ws Ws plRplRplWs gigWic gigWic gigWic'\n",
        "Counter(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5UdSOq7uA8e",
        "outputId": "be3fc4d5-b8dc-4766-c940-fa877fb07b30"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'R': 8,\n",
              "         'W': 7,\n",
              "         's': 4,\n",
              "         ' ': 6,\n",
              "         'p': 3,\n",
              "         'l': 3,\n",
              "         'g': 6,\n",
              "         'i': 6,\n",
              "         'c': 3})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Byte Pair Encoding (BPE) from scratch"
      ],
      "metadata": {
        "id": "JoMWyOeyQ7B2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¦‡ðŸ¦‡ðŸ¦‡"
      ],
      "metadata": {
        "id": "MEX_J-vjWVos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Take raw text and tokenize into characters"
      ],
      "metadata": {
        "id": "7swCSW0jTAz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making the training text longer to have more representative token statistics\n",
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "text = \"\"\"The Dark Knight Rises is a superhero movie released in 2012. It is the final part of Christopher Nolanâ€™s Dark Knight trilogy, following Batman Begins and The Dark Knight. The film stars Christian Bale as Bruce Wayne/Batman, who has been retired as Batman for eight years after the events of the previous movie.\n",
        "\n",
        "The main villain in the movie is Bane, played by Tom Hardy. Bane is a powerful and intelligent terrorist who threatens Gotham City with destruction. He forces Bruce Wayne to come out of retirement and become Batman again. Anne Hathaway plays Selina Kyle, also known as Catwoman, a skilled thief with her own agenda.\n",
        "\n",
        "The movie is about Bruce Wayneâ€™s struggle to overcome his physical and emotional challenges to save Gotham. It also shows themes of hope, sacrifice, and resilience. The film has many exciting action scenes, such as a plane hijack and a massive battle in Gotham.\n",
        "\n",
        "In the end, Batman saves the city and inspires the people of Gotham. However, he is believed to have sacrificed his life. The movie ends with a twist, suggesting that Bruce Wayne is alive and has moved on to live a quiet life.\n",
        "\n",
        "The Dark Knight Rises was a big success and is loved by many fans for its epic story, strong characters, and thrilling action.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "## For non-alphabet characters and for a more general purpose code, use this:\n",
        "#tokens = text.encode(\"utf-8\") # raw bytes\n",
        "#tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "\n",
        "# For sake of simplicity, we are only using ASCII character encoding:\n",
        "tokens = [ord(ch) for ch in text]"
      ],
      "metadata": {
        "id": "vso_LnYaQ-lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = list(tokens)  # copy so we don't destroy the original list\n"
      ],
      "metadata": {
        "id": "BtVzO_tlRqli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cc51eSTRrhd",
        "outputId": "fdc9d8db-7d2c-40c8-e48f-273f248c4eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[84, 104, 101, 32, 68, 97, 114, 107, 32, 75, 110, 105, 103, 104, 116, 32, 82, 105, 115, 101, 115, 32, 105, 115, 32, 97, 32, 115, 117, 112, 101, 114, 104, 101, 114, 111, 32, 109, 111, 118, 105, 101, 32, 114, 101, 108, 101, 97, 115, 101, 100, 32, 105, 110, 32, 50, 48, 49, 50, 46, 32, 73, 116, 32, 105, 115, 32, 116, 104, 101, 32, 102, 105, 110, 97, 108, 32, 112, 97, 114, 116, 32, 111, 102, 32, 67, 104, 114, 105, 115, 116, 111, 112, 104, 101, 114, 32, 78, 111, 108, 97, 110, 8217, 115, 32, 68, 97, 114, 107, 32, 75, 110, 105, 103, 104, 116, 32, 116, 114, 105, 108, 111, 103, 121, 44, 32, 102, 111, 108, 108, 111, 119, 105, 110, 103, 32, 66, 97, 116, 109, 97, 110, 32, 66, 101, 103, 105, 110, 115, 32, 97, 110, 100, 32, 84, 104, 101, 32, 68, 97, 114, 107, 32, 75, 110, 105, 103, 104, 116, 46, 32, 84, 104, 101, 32, 102, 105, 108, 109, 32, 115, 116, 97, 114, 115, 32, 67, 104, 114, 105, 115, 116, 105, 97, 110, 32, 66, 97, 108, 101, 32, 97, 115, 32, 66, 114, 117, 99, 101, 32, 87, 97, 121, 110, 101, 47, 66, 97, 116, 109, 97, 110, 44, 32, 119, 104, 111, 32, 104, 97, 115, 32, 98, 101, 101, 110, 32, 114, 101, 116, 105, 114, 101, 100, 32, 97, 115, 32, 66, 97, 116, 109, 97, 110, 32, 102, 111, 114, 32, 101, 105, 103, 104, 116, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 116, 104, 101, 32, 101, 118, 101, 110, 116, 115, 32, 111, 102, 32, 116, 104, 101, 32, 112, 114, 101, 118, 105, 111, 117, 115, 32, 109, 111, 118, 105, 101, 46, 10, 10, 84, 104, 101, 32, 109, 97, 105, 110, 32, 118, 105, 108, 108, 97, 105, 110, 32, 105, 110, 32, 116, 104, 101, 32, 109, 111, 118, 105, 101, 32, 105, 115, 32, 66, 97, 110, 101, 44, 32, 112, 108, 97, 121, 101, 100, 32, 98, 121, 32, 84, 111, 109, 32, 72, 97, 114, 100, 121, 46, 32, 66, 97, 110, 101, 32, 105, 115, 32, 97, 32, 112, 111, 119, 101, 114, 102, 117, 108, 32, 97, 110, 100, 32, 105, 110, 116, 101, 108, 108, 105, 103, 101, 110, 116, 32, 116, 101, 114, 114, 111, 114, 105, 115, 116, 32, 119, 104, 111, 32, 116, 104, 114, 101, 97, 116, 101, 110, 115, 32, 71, 111, 116, 104, 97, 109, 32, 67, 105, 116, 121, 32, 119, 105, 116, 104, 32, 100, 101, 115, 116, 114, 117, 99, 116, 105, 111, 110, 46, 32, 72, 101, 32, 102, 111, 114, 99, 101, 115, 32, 66, 114, 117, 99, 101, 32, 87, 97, 121, 110, 101, 32, 116, 111, 32, 99, 111, 109, 101, 32, 111, 117, 116, 32, 111, 102, 32, 114, 101, 116, 105, 114, 101, 109, 101, 110, 116, 32, 97, 110, 100, 32, 98, 101, 99, 111, 109, 101, 32, 66, 97, 116, 109, 97, 110, 32, 97, 103, 97, 105, 110, 46, 32, 65, 110, 110, 101, 32, 72, 97, 116, 104, 97, 119, 97, 121, 32, 112, 108, 97, 121, 115, 32, 83, 101, 108, 105, 110, 97, 32, 75, 121, 108, 101, 44, 32, 97, 108, 115, 111, 32, 107, 110, 111, 119, 110, 32, 97, 115, 32, 67, 97, 116, 119, 111, 109, 97, 110, 44, 32, 97, 32, 115, 107, 105, 108, 108, 101, 100, 32, 116, 104, 105, 101, 102, 32, 119, 105, 116, 104, 32, 104, 101, 114, 32, 111, 119, 110, 32, 97, 103, 101, 110, 100, 97, 46, 10, 10, 84, 104, 101, 32, 109, 111, 118, 105, 101, 32, 105, 115, 32, 97, 98, 111, 117, 116, 32, 66, 114, 117, 99, 101, 32, 87, 97, 121, 110, 101, 8217, 115, 32, 115, 116, 114, 117, 103, 103, 108, 101, 32, 116, 111, 32, 111, 118, 101, 114, 99, 111, 109, 101, 32, 104, 105, 115, 32, 112, 104, 121, 115, 105, 99, 97, 108, 32, 97, 110, 100, 32, 101, 109, 111, 116, 105, 111, 110, 97, 108, 32, 99, 104, 97, 108, 108, 101, 110, 103, 101, 115, 32, 116, 111, 32, 115, 97, 118, 101, 32, 71, 111, 116, 104, 97, 109, 46, 32, 73, 116, 32, 97, 108, 115, 111, 32, 115, 104, 111, 119, 115, 32, 116, 104, 101, 109, 101, 115, 32, 111, 102, 32, 104, 111, 112, 101, 44, 32, 115, 97, 99, 114, 105, 102, 105, 99, 101, 44, 32, 97, 110, 100, 32, 114, 101, 115, 105, 108, 105, 101, 110, 99, 101, 46, 32, 84, 104, 101, 32, 102, 105, 108, 109, 32, 104, 97, 115, 32, 109, 97, 110, 121, 32, 101, 120, 99, 105, 116, 105, 110, 103, 32, 97, 99, 116, 105, 111, 110, 32, 115, 99, 101, 110, 101, 115, 44, 32, 115, 117, 99, 104, 32, 97, 115, 32, 97, 32, 112, 108, 97, 110, 101, 32, 104, 105, 106, 97, 99, 107, 32, 97, 110, 100, 32, 97, 32, 109, 97, 115, 115, 105, 118, 101, 32, 98, 97, 116, 116, 108, 101, 32, 105, 110, 32, 71, 111, 116, 104, 97, 109, 46, 10, 10, 73, 110, 32, 116, 104, 101, 32, 101, 110, 100, 44, 32, 66, 97, 116, 109, 97, 110, 32, 115, 97, 118, 101, 115, 32, 116, 104, 101, 32, 99, 105, 116, 121, 32, 97, 110, 100, 32, 105, 110, 115, 112, 105, 114, 101, 115, 32, 116, 104, 101, 32, 112, 101, 111, 112, 108, 101, 32, 111, 102, 32, 71, 111, 116, 104, 97, 109, 46, 32, 72, 111, 119, 101, 118, 101, 114, 44, 32, 104, 101, 32, 105, 115, 32, 98, 101, 108, 105, 101, 118, 101, 100, 32, 116, 111, 32, 104, 97, 118, 101, 32, 115, 97, 99, 114, 105, 102, 105, 99, 101, 100, 32, 104, 105, 115, 32, 108, 105, 102, 101, 46, 32, 84, 104, 101, 32, 109, 111, 118, 105, 101, 32, 101, 110, 100, 115, 32, 119, 105, 116, 104, 32, 97, 32, 116, 119, 105, 115, 116, 44, 32, 115, 117, 103, 103, 101, 115, 116, 105, 110, 103, 32, 116, 104, 97, 116, 32, 66, 114, 117, 99, 101, 32, 87, 97, 121, 110, 101, 32, 105, 115, 32, 97, 108, 105, 118, 101, 32, 97, 110, 100, 32, 104, 97, 115, 32, 109, 111, 118, 101, 100, 32, 111, 110, 32, 116, 111, 32, 108, 105, 118, 101, 32, 97, 32, 113, 117, 105, 101, 116, 32, 108, 105, 102, 101, 46, 10, 10, 84, 104, 101, 32, 68, 97, 114, 107, 32, 75, 110, 105, 103, 104, 116, 32, 82, 105, 115, 101, 115, 32, 119, 97, 115, 32, 97, 32, 98, 105, 103, 32, 115, 117, 99, 99, 101, 115, 115, 32, 97, 110, 100, 32, 105, 115, 32, 108, 111, 118, 101, 100, 32, 98, 121, 32, 109, 97, 110, 121, 32, 102, 97, 110, 115, 32, 102, 111, 114, 32, 105, 116, 115, 32, 101, 112, 105, 99, 32, 115, 116, 111, 114, 121, 44, 32, 115, 116, 114, 111, 110, 103, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 44, 32, 97, 110, 100, 32, 116, 104, 114, 105, 108, 108, 105, 110, 103, 32, 97, 99, 116, 105, 111, 110, 46, 10, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Write a function to count the frequency of the adjacent pairs of characters"
      ],
      "metadata": {
        "id": "EhtGo95DVW58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Count all adjacent pairs in our current sequence 'ids'.\n",
        "\n",
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "stats = get_stats(ids)\n",
        "print(stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwQ_jZmJR0Am",
        "outputId": "16be7b61-e52b-4655-cde6-cc79f62c440d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(84, 104): 8, (104, 101): 20, (101, 32): 42, (32, 68): 4, (68, 97): 4, (97, 114): 9, (114, 107): 4, (107, 32): 5, (32, 75): 5, (75, 110): 4, (110, 105): 4, (105, 103): 7, (103, 104): 5, (104, 116): 5, (116, 32): 14, (32, 82): 2, (82, 105): 2, (105, 115): 16, (115, 101): 3, (101, 115): 12, (115, 32): 39, (32, 105): 14, (32, 97): 31, (97, 32): 9, (32, 115): 15, (115, 117): 4, (117, 112): 1, (112, 101): 3, (101, 114): 10, (114, 104): 1, (114, 111): 3, (111, 32): 10, (32, 109): 10, (109, 111): 7, (111, 118): 8, (118, 105): 7, (105, 101): 9, (32, 114): 4, (114, 101): 9, (101, 108): 4, (108, 101): 8, (101, 97): 3, (97, 115): 10, (101, 100): 8, (100, 32): 18, (105, 110): 15, (110, 32): 16, (32, 50): 1, (50, 48): 1, (48, 49): 1, (49, 50): 1, (50, 46): 1, (46, 32): 9, (32, 73): 2, (73, 116): 2, (32, 116): 20, (116, 104): 20, (32, 102): 8, (102, 105): 5, (110, 97): 3, (97, 108): 8, (108, 32): 4, (32, 112): 8, (112, 97): 1, (114, 116): 1, (32, 111): 9, (111, 102): 5, (102, 32): 6, (32, 67): 4, (67, 104): 2, (104, 114): 4, (114, 105): 7, (115, 116): 10, (116, 111): 7, (111, 112): 3, (112, 104): 2, (114, 32): 5, (32, 78): 1, (78, 111): 1, (111, 108): 2, (108, 97): 5, (97, 110): 24, (110, 8217): 1, (8217, 115): 2, (116, 114): 4, (105, 108): 7, (108, 111): 3, (111, 103): 1, (103, 121): 1, (121, 44): 2, (44, 32): 13, (102, 111): 4, (108, 108): 6, (111, 119): 6, (119, 105): 5, (110, 103): 6, (103, 32): 6, (32, 66): 12, (66, 97): 8, (97, 116): 10, (116, 109): 5, (109, 97): 10, (66, 101): 1, (101, 103): 1, (103, 105): 1, (110, 115): 4, (110, 100): 13, (32, 84): 5, (116, 46): 1, (108, 109): 2, (109, 32): 4, (116, 97): 1, (114, 115): 3, (116, 105): 9, (105, 97): 1, (66, 114): 4, (114, 117): 6, (117, 99): 7, (99, 101): 10, (32, 87): 4, (87, 97): 4, (97, 121): 7, (121, 110): 4, (110, 101): 9, (101, 47): 1, (47, 66): 1, (110, 44): 2, (32, 119): 6, (119, 104): 2, (104, 111): 4, (32, 104): 10, (104, 97): 12, (32, 98): 7, (98, 101): 3, (101, 101): 1, (101, 110): 11, (101, 116): 3, (105, 114): 3, (111, 114): 5, (32, 101): 7, (101, 105): 1, (32, 121): 1, (121, 101): 2, (97, 102): 1, (102, 116): 1, (116, 101): 5, (101, 118): 4, (118, 101): 12, (110, 116): 4, (116, 115): 2, (112, 114): 1, (105, 111): 5, (111, 117): 3, (117, 115): 1, (101, 46): 4, (46, 10): 5, (10, 10): 5, (10, 84): 3, (97, 105): 3, (32, 118): 1, (101, 44): 4, (112, 108): 4, (98, 121): 2, (121, 32): 7, (84, 111): 1, (111, 109): 5, (32, 72): 4, (72, 97): 2, (114, 100): 1, (100, 121): 1, (121, 46): 1, (112, 111): 1, (119, 101): 2, (114, 102): 1, (102, 117): 1, (117, 108): 1, (108, 105): 9, (103, 101): 4, (114, 114): 1, (32, 71): 4, (71, 111): 4, (111, 116): 5, (97, 109): 4, (67, 105): 1, (105, 116): 7, (116, 121): 2, (104, 32): 4, (32, 100): 1, (100, 101): 1, (99, 116): 4, (111, 110): 6, (110, 46): 3, (72, 101): 1, (114, 99): 2, (32, 99): 4, (99, 111): 3, (109, 101): 5, (117, 116): 2, (101, 109): 3, (101, 99): 1, (97, 103): 2, (103, 97): 1, (32, 65): 1, (65, 110): 1, (110, 110): 1, (97, 119): 1, (119, 97): 2, (121, 115): 2, (32, 83): 1, (83, 101): 1, (75, 121): 1, (121, 108): 1, (108, 115): 2, (115, 111): 2, (32, 107): 1, (107, 110): 1, (110, 111): 1, (119, 110): 2, (67, 97): 1, (116, 119): 2, (119, 111): 1, (115, 107): 1, (107, 105): 1, (104, 105): 4, (101, 102): 1, (100, 97): 1, (97, 46): 1, (97, 98): 1, (98, 111): 1, (101, 8217): 1, (117, 103): 2, (103, 103): 2, (103, 108): 1, (104, 121): 1, (115, 105): 3, (105, 99): 4, (99, 97): 1, (99, 104): 3, (115, 97): 4, (97, 118): 3, (109, 46): 3, (115, 104): 1, (119, 115): 1, (97, 99): 6, (99, 114): 2, (105, 102): 4, (110, 99): 1, (110, 121): 2, (101, 120): 1, (120, 99): 1, (99, 105): 2, (115, 99): 1, (115, 44): 2, (105, 106): 1, (106, 97): 1, (99, 107): 1, (115, 115): 2, (105, 118): 3, (98, 97): 1, (116, 116): 1, (116, 108): 1, (10, 73): 1, (73, 110): 1, (100, 44): 1, (115, 112): 1, (112, 105): 2, (101, 111): 1, (72, 111): 1, (114, 44): 1, (32, 108): 4, (102, 101): 2, (100, 115): 1, (116, 44): 1, (32, 113): 1, (113, 117): 1, (117, 105): 1, (98, 105): 1, (99, 99): 1, (102, 97): 1, (101, 112): 1, (99, 32): 1, (114, 121): 1, (114, 97): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Select the pair with the highest frequency"
      ],
      "metadata": {
        "id": "-voL-MOdVdAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Select the pair with the highest frequency.\n",
        "pair = max(stats, key=stats.get)\n",
        "print(pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THZ0crZlR-dC",
        "outputId": "a930ff79-95e1-46ce-caea-abdf17e83fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(101, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Define the new token's ID (ID of the merged token is added to vocabulary)"
      ],
      "metadata": {
        "id": "Pzhu1x73Viqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Define the new token's ID as 256 + i.\n",
        "i = 0\n",
        "idx = 128 + i\n",
        "print(idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saZaHgdXSIP6",
        "outputId": "f496967d-652b-4a79-879b-782364720f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For readability, decode the original token IDs (pair[0], pair[1]) into characters\n",
        "    # just for a nice printout. (Assumes these IDs map to ASCII, etc.)\n",
        "char_pair = (chr(pair[0]), chr(pair[1]))\n",
        "print(char_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwFZNOtSSXJJ",
        "outputId": "80f1bd23-4b58-4958-ec30-4a988dfa528a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('e', ' ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Show which pair we are merging"
      ],
      "metadata": {
        "id": "F1uL6Et9V6_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show which pair we are merging.\n",
        "print(f\"merging {pair} ({char_pair[0]}{char_pair[1]}) into a new token {idx}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjLr4KISSfF-",
        "outputId": "899b8f06-746c-4cd0-8383-e2e525641527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (101, 32) (e ) into a new token 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Peform the merge: replace all occurences of the most frequent pair with the new token ID"
      ],
      "metadata": {
        "id": "7npSNo-dV-YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Perform the merge, replacing all occurrences of 'pair' with 'idx'.\n",
        "def merge(ids, pair, idx):\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "ids = merge(ids, pair, idx)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHw5ObGoSmcG",
        "outputId": "09f8f1da-ab74-4b18-ec85-a3d267b7cbc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[84, 104, 256, 68, 97, 114, 107, 32, 75, 110, 105, 103, 104, 116, 32, 82, 105, 115, 101, 115, 32, 105, 115, 32, 97, 32, 115, 117, 112, 101, 114, 104, 101, 114, 111, 32, 109, 111, 118, 105, 256, 114, 101, 108, 101, 97, 115, 101, 100, 32, 105, 110, 32, 50, 48, 49, 50, 46, 32, 73, 116, 32, 105, 115, 32, 116, 104, 256, 102, 105, 110, 97, 108, 32, 112, 97, 114, 116, 32, 111, 102, 32, 67, 104, 114, 105, 115, 116, 111, 112, 104, 101, 114, 32, 78, 111, 108, 97, 110, 8217, 115, 32, 68, 97, 114, 107, 32, 75, 110, 105, 103, 104, 116, 32, 116, 114, 105, 108, 111, 103, 121, 44, 32, 102, 111, 108, 108, 111, 119, 105, 110, 103, 32, 66, 97, 116, 109, 97, 110, 32, 66, 101, 103, 105, 110, 115, 32, 97, 110, 100, 32, 84, 104, 256, 68, 97, 114, 107, 32, 75, 110, 105, 103, 104, 116, 46, 32, 84, 104, 256, 102, 105, 108, 109, 32, 115, 116, 97, 114, 115, 32, 67, 104, 114, 105, 115, 116, 105, 97, 110, 32, 66, 97, 108, 256, 97, 115, 32, 66, 114, 117, 99, 256, 87, 97, 121, 110, 101, 47, 66, 97, 116, 109, 97, 110, 44, 32, 119, 104, 111, 32, 104, 97, 115, 32, 98, 101, 101, 110, 32, 114, 101, 116, 105, 114, 101, 100, 32, 97, 115, 32, 66, 97, 116, 109, 97, 110, 32, 102, 111, 114, 32, 101, 105, 103, 104, 116, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 116, 104, 256, 101, 118, 101, 110, 116, 115, 32, 111, 102, 32, 116, 104, 256, 112, 114, 101, 118, 105, 111, 117, 115, 32, 109, 111, 118, 105, 101, 46, 10, 10, 84, 104, 256, 109, 97, 105, 110, 32, 118, 105, 108, 108, 97, 105, 110, 32, 105, 110, 32, 116, 104, 256, 109, 111, 118, 105, 256, 105, 115, 32, 66, 97, 110, 101, 44, 32, 112, 108, 97, 121, 101, 100, 32, 98, 121, 32, 84, 111, 109, 32, 72, 97, 114, 100, 121, 46, 32, 66, 97, 110, 256, 105, 115, 32, 97, 32, 112, 111, 119, 101, 114, 102, 117, 108, 32, 97, 110, 100, 32, 105, 110, 116, 101, 108, 108, 105, 103, 101, 110, 116, 32, 116, 101, 114, 114, 111, 114, 105, 115, 116, 32, 119, 104, 111, 32, 116, 104, 114, 101, 97, 116, 101, 110, 115, 32, 71, 111, 116, 104, 97, 109, 32, 67, 105, 116, 121, 32, 119, 105, 116, 104, 32, 100, 101, 115, 116, 114, 117, 99, 116, 105, 111, 110, 46, 32, 72, 256, 102, 111, 114, 99, 101, 115, 32, 66, 114, 117, 99, 256, 87, 97, 121, 110, 256, 116, 111, 32, 99, 111, 109, 256, 111, 117, 116, 32, 111, 102, 32, 114, 101, 116, 105, 114, 101, 109, 101, 110, 116, 32, 97, 110, 100, 32, 98, 101, 99, 111, 109, 256, 66, 97, 116, 109, 97, 110, 32, 97, 103, 97, 105, 110, 46, 32, 65, 110, 110, 256, 72, 97, 116, 104, 97, 119, 97, 121, 32, 112, 108, 97, 121, 115, 32, 83, 101, 108, 105, 110, 97, 32, 75, 121, 108, 101, 44, 32, 97, 108, 115, 111, 32, 107, 110, 111, 119, 110, 32, 97, 115, 32, 67, 97, 116, 119, 111, 109, 97, 110, 44, 32, 97, 32, 115, 107, 105, 108, 108, 101, 100, 32, 116, 104, 105, 101, 102, 32, 119, 105, 116, 104, 32, 104, 101, 114, 32, 111, 119, 110, 32, 97, 103, 101, 110, 100, 97, 46, 10, 10, 84, 104, 256, 109, 111, 118, 105, 256, 105, 115, 32, 97, 98, 111, 117, 116, 32, 66, 114, 117, 99, 256, 87, 97, 121, 110, 101, 8217, 115, 32, 115, 116, 114, 117, 103, 103, 108, 256, 116, 111, 32, 111, 118, 101, 114, 99, 111, 109, 256, 104, 105, 115, 32, 112, 104, 121, 115, 105, 99, 97, 108, 32, 97, 110, 100, 32, 101, 109, 111, 116, 105, 111, 110, 97, 108, 32, 99, 104, 97, 108, 108, 101, 110, 103, 101, 115, 32, 116, 111, 32, 115, 97, 118, 256, 71, 111, 116, 104, 97, 109, 46, 32, 73, 116, 32, 97, 108, 115, 111, 32, 115, 104, 111, 119, 115, 32, 116, 104, 101, 109, 101, 115, 32, 111, 102, 32, 104, 111, 112, 101, 44, 32, 115, 97, 99, 114, 105, 102, 105, 99, 101, 44, 32, 97, 110, 100, 32, 114, 101, 115, 105, 108, 105, 101, 110, 99, 101, 46, 32, 84, 104, 256, 102, 105, 108, 109, 32, 104, 97, 115, 32, 109, 97, 110, 121, 32, 101, 120, 99, 105, 116, 105, 110, 103, 32, 97, 99, 116, 105, 111, 110, 32, 115, 99, 101, 110, 101, 115, 44, 32, 115, 117, 99, 104, 32, 97, 115, 32, 97, 32, 112, 108, 97, 110, 256, 104, 105, 106, 97, 99, 107, 32, 97, 110, 100, 32, 97, 32, 109, 97, 115, 115, 105, 118, 256, 98, 97, 116, 116, 108, 256, 105, 110, 32, 71, 111, 116, 104, 97, 109, 46, 10, 10, 73, 110, 32, 116, 104, 256, 101, 110, 100, 44, 32, 66, 97, 116, 109, 97, 110, 32, 115, 97, 118, 101, 115, 32, 116, 104, 256, 99, 105, 116, 121, 32, 97, 110, 100, 32, 105, 110, 115, 112, 105, 114, 101, 115, 32, 116, 104, 256, 112, 101, 111, 112, 108, 256, 111, 102, 32, 71, 111, 116, 104, 97, 109, 46, 32, 72, 111, 119, 101, 118, 101, 114, 44, 32, 104, 256, 105, 115, 32, 98, 101, 108, 105, 101, 118, 101, 100, 32, 116, 111, 32, 104, 97, 118, 256, 115, 97, 99, 114, 105, 102, 105, 99, 101, 100, 32, 104, 105, 115, 32, 108, 105, 102, 101, 46, 32, 84, 104, 256, 109, 111, 118, 105, 256, 101, 110, 100, 115, 32, 119, 105, 116, 104, 32, 97, 32, 116, 119, 105, 115, 116, 44, 32, 115, 117, 103, 103, 101, 115, 116, 105, 110, 103, 32, 116, 104, 97, 116, 32, 66, 114, 117, 99, 256, 87, 97, 121, 110, 256, 105, 115, 32, 97, 108, 105, 118, 256, 97, 110, 100, 32, 104, 97, 115, 32, 109, 111, 118, 101, 100, 32, 111, 110, 32, 116, 111, 32, 108, 105, 118, 256, 97, 32, 113, 117, 105, 101, 116, 32, 108, 105, 102, 101, 46, 10, 10, 84, 104, 256, 68, 97, 114, 107, 32, 75, 110, 105, 103, 104, 116, 32, 82, 105, 115, 101, 115, 32, 119, 97, 115, 32, 97, 32, 98, 105, 103, 32, 115, 117, 99, 99, 101, 115, 115, 32, 97, 110, 100, 32, 105, 115, 32, 108, 111, 118, 101, 100, 32, 98, 121, 32, 109, 97, 110, 121, 32, 102, 97, 110, 115, 32, 102, 111, 114, 32, 105, 116, 115, 32, 101, 112, 105, 99, 32, 115, 116, 111, 114, 121, 44, 32, 115, 116, 114, 111, 110, 103, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 115, 44, 32, 97, 110, 100, 32, 116, 104, 114, 105, 108, 108, 105, 110, 103, 32, 97, 99, 116, 105, 111, 110, 46, 10, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Write all functions together and define number of iterations to run.\n",
        "\n",
        "Here, we have to select how many merges we do. If we do 20 merges, the vocabulary size increases from 128 to 148."
      ],
      "metadata": {
        "id": "gOhi9-76WJsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "# ---\n",
        "vocab_size = 148  # the desired final vocabulary size\n",
        "num_merges = vocab_size - 128\n",
        "ids = list(tokens)  # copy so we don't destroy the original list\n",
        "\n",
        "merges = {}  # (int, int) -> int\n",
        "for i in range(num_merges):\n",
        "    # 1) Count all adjacent pairs in our current sequence 'ids'.\n",
        "    stats = get_stats(ids)\n",
        "    pair = max(stats, key=stats.get)\n",
        "    idx = 128 + i\n",
        "    # Decode the characters of the pair for display\n",
        "    char_pair = (chr(pair[0]), chr(pair[1]))\n",
        "    print(f\"merging {pair} ({char_pair[0]}{char_pair[1]}) into a new token {idx}\")\n",
        "    ids = merge(ids, pair, idx)\n",
        "    merges[pair] = idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE5Hq2TARB69",
        "outputId": "43db1258-704f-4a3f-e139-ab3344dd2aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (101, 32) (e ) into a new token 128\n",
            "merging (115, 32) (s ) into a new token 129\n",
            "merging (97, 110) (an) into a new token 130\n",
            "merging (116, 104) (th) into a new token 131\n",
            "merging (100, 32) (d ) into a new token 132\n",
            "merging (105, 110) (in) into a new token 133\n",
            "merging (116, 32) (t ) into a new token 134\n",
            "merging (32, 115) ( s) into a new token 135\n",
            "merging (101, 110) (en) into a new token 136\n",
            "merging (105, 129) (iÂ) into a new token 137\n",
            "merging (101, 114) (er) into a new token 138\n",
            "merging (130, 132) (Â‚Â„) into a new token 139\n",
            "merging (104, 128) (hÂ€) into a new token 140\n",
            "merging (97, 114) (ar) into a new token 141\n",
            "merging (114, 101) (re) into a new token 142\n",
            "merging (46, 32) (. ) into a new token 143\n",
            "merging (44, 32) (, ) into a new token 144\n",
            "merging (84, 140) (TÂŒ) into a new token 145\n",
            "merging (111, 32) (o ) into a new token 146\n",
            "merging (111, 118) (ov) into a new token 147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(ids))\n",
        "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_ZnnFtIRle_",
        "outputId": "a9411781-e927-474a-913b-b3cc8403e921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens length: 1248\n",
            "ids length: 953\n",
            "compression ratio: 1.31X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In class activity: Take any text of your choice and implement the BPE algorithm. Report the compression ratio achieved."
      ],
      "metadata": {
        "id": "Vn555CG7XW7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the tiktoken library"
      ],
      "metadata": {
        "id": "lHJT8Uczcngk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tiktoken\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "# Text to encode and decode\n",
        "text = \"The lion roams in the jungle\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1. GPT-2 Encoding/Decoding\n",
        "#    Using the \"gpt2\" encoding\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Encode: text -> list of token IDs\n",
        "token_ids_gpt2 = tokenizer_gpt2.encode(text)\n",
        "\n",
        "# Decode: list of token IDs -> original text (just to verify correctness)\n",
        "decoded_text_gpt2 = tokenizer_gpt2.decode(token_ids_gpt2)\n",
        "\n",
        "# We can also get each token string by decoding the IDs one by one\n",
        "tokens_gpt2 = [tokenizer_gpt2.decode([tid]) for tid in token_ids_gpt2]\n",
        "\n",
        "print(\"=== GPT-2 Encoding ===\")\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Token IDs:     \", token_ids_gpt2)\n",
        "print(\"Tokens:        \", tokens_gpt2)\n",
        "print(\"Decoded Text:  \", decoded_text_gpt2)\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUm2SdUcXblG",
        "outputId": "53df028f-b27a-4ef0-d162-c88c4ad6b492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "=== GPT-2 Encoding ===\n",
            "Original Text:  The lion roams in the jungle\n",
            "Token IDs:      [464, 18744, 686, 4105, 287, 262, 20712]\n",
            "Tokens:         ['The', ' lion', ' ro', 'ams', ' in', ' the', ' jungle']\n",
            "Decoded Text:   The lion roams in the jungle\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2. GPT-3.5 Encoding\n",
        "#    Using the encoding_for_model(\"gpt-3.5-turbo\")\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tokenizer_gpt35 = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "token_ids_gpt35 = tokenizer_gpt35.encode(text)\n",
        "decoded_text_gpt35 = tokenizer_gpt35.decode(token_ids_gpt35)\n",
        "tokens_gpt35 = [tokenizer_gpt35.decode([tid]) for tid in token_ids_gpt35]\n",
        "\n",
        "print(\"=== GPT-3.5 Encoding ===\")\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Token IDs:     \", token_ids_gpt35)\n",
        "print(\"Tokens:        \", tokens_gpt35)\n",
        "print(\"Decoded Text:  \", decoded_text_gpt35)\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVHkblCLc2mG",
        "outputId": "d56cba04-60f0-4898-fb36-eed614c6956c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPT-3.5 Encoding ===\n",
            "Original Text:  The lion roams in the jungle\n",
            "Token IDs:      [791, 40132, 938, 4214, 304, 279, 45520]\n",
            "Tokens:         ['The', ' lion', ' ro', 'ams', ' in', ' the', ' jungle']\n",
            "Decoded Text:   The lion roams in the jungle\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 3. GPT-4 Encoding\n",
        "#    Using the encoding_for_model(\"gpt-4\")\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tokenizer_gpt4 = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "\n",
        "token_ids_gpt4 = tokenizer_gpt4.encode(text)\n",
        "decoded_text_gpt4 = tokenizer_gpt4.decode(token_ids_gpt4)\n",
        "tokens_gpt4 = [tokenizer_gpt4.decode([tid]) for tid in token_ids_gpt4]\n",
        "\n",
        "print(\"=== GPT-4 Encoding ===\")\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Token IDs:     \", token_ids_gpt4)\n",
        "print(\"Tokens:        \", tokens_gpt4)\n",
        "print(\"Decoded Text:  \", decoded_text_gpt4)"
      ],
      "metadata": {
        "id": "wnhXtYvwc3PF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
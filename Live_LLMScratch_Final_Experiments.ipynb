{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amankiitg/GenAI/blob/main/Live_LLMScratch_Final_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfTQC5-R9KkG"
      },
      "source": [
        "# BUILD A LARGE LANGUAGE MODEL FROM SCRATCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1rpOiY49KkH"
      },
      "source": [
        "### STEP 1: LOADING THE DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets"
      ],
      "metadata": {
        "id": "sq93_ZehH6fV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsPpTAotu7gK",
        "outputId": "f65372ad-9e95-4301-ed70-34604ddc1e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'pg76057.txt' loaded with 664622 characters.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# from datasets import load_dataset\n",
        "\n",
        "file_path = \"pg76057.txt\"\n",
        "\n",
        "# Check if the file exists in the current directory (i.e., Colab session)\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "    print(f\"File '{file_path}' loaded with {len(text_data)} characters.\")\n",
        "else:\n",
        "    # Load a subset of The Pile (e.g., StackExchange, Books3, etc.)\n",
        "    # Load a small subset for quick testing\n",
        "    dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T\", \"wikipedia\", split=\"train[:1%]\")\n",
        "\n",
        "\n",
        "    # Concatenate all text fields into one giant string\n",
        "    text_data = \"\\n\".join([item[\"text\"] for item in dataset if item[\"text\"] is not None])\n",
        "\n",
        "    text_data = text_data[:5_000]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0g39tUJ9KkI"
      },
      "source": [
        "### STEP 2: IMPLEMENTING THE TOKENIZER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eAStBoVS9KkI"
      },
      "outputs": [],
      "source": [
        "!pip3 install tiktoken > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvvZUNnH9KkI",
        "outputId": "a93c0828-8212-4f86-f6ba-adae6c7d9a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4zPCCnGu9KkJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BLKO6Xh9KkJ",
        "outputId": "e03e3af4-5963-4dcd-8f8c-3c2207fbc800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdUX0CEb9KkJ",
        "outputId": "4708d3ca-b207-44fc-a96b-a863516a909a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ],
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayGpRaun91ku",
        "outputId": "a7e7af07-6e26-409a-8e32-79bfb00d8ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 664622\n",
            "Tokens: 185320\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVpgQJxO9KkJ"
      },
      "source": [
        "### STEP 3: CREATING INPUT-TARGET PAIRS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O-UdNDW59KkJ"
      },
      "outputs": [],
      "source": [
        "with open(\"pg76057.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# # Load a subset of The Pile (e.g., StackExchange, Books3, etc.)\n",
        "# dataset = load_dataset(\"EleutherAI/pile\", \"stackexchange\", split='train[:1%]')\n",
        "\n",
        "# # Concatenate all text fields into one giant string\n",
        "# raw_text = \"\\n\".join([item[\"text\"] for item in dataset if item[\"text\"] is not None])\n",
        "\n",
        "# enc_text = tokenizer.encode(raw_text)\n",
        "# print(len(enc_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UvwzHMOM9KkJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FA3xjMcL9KkJ"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8QvsSIs9KkJ",
        "outputId": "e399dd7f-8976-489a-8309-f7ff2ffff4b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "[tensor([[171, 119, 123, 464]]), tensor([[ 119,  123,  464, 4935]])]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PFNk_Q9KkJ"
      },
      "source": [
        "### STEP 4: CREATING TOKEN EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "r59Haayq9KkJ"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0Ec72YAl9KkJ"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYpQsV1c9KkK",
        "outputId": "e937c024-a1c6-4238-fdee-f7093dae5f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[  171,   119,   123,   464],\n",
            "        [ 4935, 20336, 46566,   286],\n",
            "        [ 3576, 14860,   290, 25476],\n",
            "        [  198,   220,   220,   220],\n",
            "        [  220,   198,  1212, 47179],\n",
            "        [  318,   329,   262,   779],\n",
            "        [  286,  2687,  6609,   287],\n",
            "        [  262,  1578,  1829,   290]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZrkE2Qm9KkK",
        "outputId": "fc6e9abb-794b-4ced-cf14-616701fc21fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8smgXdd9KkK"
      },
      "source": [
        "### STEP 5: CREATING POSITIONAL EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7RwqymHS9KkK"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjcgOLZB9KkK",
        "outputId": "2a6fc20b-b89e-402f-87e6-c88bbe8cf592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSaYi0av9KkK"
      },
      "source": [
        "### STEP 6: CREATING INPUT EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSeEO2BR9KkK",
        "outputId": "9f6611c3-98a0-4bb5-8951-8c12cff0886c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cddJBU79KkK"
      },
      "source": [
        "### STEP 7: IMPLEMENTING MULTI-HEAD ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bOK38nTO9KkK"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPNlDRLg9KkK",
        "outputId": "32b19a2c-a3a1-4646-8c28-0e0cfdb1dea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the tensor with 3 rows and 6 columns\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
        "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
        "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9nmF0vx9KkK"
      },
      "source": [
        "### STEP 8: IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0M8o78gL9KkK"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBIbfH59KkK"
      },
      "source": [
        "### STEP 9: THE BUILDING BLOCKS-LAYER NORMALIZATION, GELU AND FEED-FORWARD NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "43IkKn1t9KkK"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Cc7qPADt9KkK"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "\n",
        "        self.use_norm = cfg.get('use_norm', True)\n",
        "        self.use_residual = cfg.get('use_residual', True)\n",
        "        self.use_ffn = cfg.get('use_ffn', True)\n",
        "\n",
        "        # self.ff = FeedForward(cfg)\n",
        "        # self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        # self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "\n",
        "        self.ff = FeedForward(cfg) if self.use_ffn else nn.Identity()\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"]) if self.use_norm else nn.Identity()\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"]) if self.use_norm else nn.Identity()\n",
        "\n",
        "\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "\n",
        "        if self.use_residual:\n",
        "            x = x + shortcut\n",
        "        # x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        if self.use_residual:\n",
        "            x = x + shortcut\n",
        "        # x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw0QKZOa9KkL"
      },
      "source": [
        "### STEP 10: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EMfj2tEz9KkL"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKB4thW19KkL",
        "outputId": "c0f9f850-d9d1-4ba9-9b8b-bf7f963606a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.4708,  0.5737, -0.5967,  ...,  0.2019, -0.5665,  0.1800],\n",
            "         [-0.3895, -0.1978, -0.8885,  ...,  0.2242, -1.2341,  0.1752],\n",
            "         [ 0.6973, -0.3432, -0.6080,  ...,  0.3747, -0.6967,  0.1088],\n",
            "         [-0.2962, -0.6957, -1.1371,  ...,  0.3579,  0.3058, -0.2915]],\n",
            "\n",
            "        [[-0.1514,  0.3329, -0.9740,  ..., -0.1368, -0.6974, -0.1851],\n",
            "         [-0.4894, -0.3492, -0.9759,  ...,  0.2951, -0.3396,  0.2109],\n",
            "         [ 0.5082, -0.1425,  0.2549,  ...,  0.1618,  0.1304, -0.3092],\n",
            "         [-0.4146, -0.0514, -0.5187,  ..., -0.1869, -0.1303, -0.4969]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQOyQ3tl9KkL",
        "outputId": "7e8227f6-b5ac-4979-96f6-d746c9e6240f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 162,419,712\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2obfC_B9KkL",
        "outputId": "a46e1a32-3031-4ba4-a8fc-ae292e526a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITBHaII99KkL",
        "outputId": "597271e1-cf2f-4357-fbd3-175977c746f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 619.58 MB\n"
          ]
        }
      ],
      "source": [
        "total_size_bytes = total_params * 4 #A\n",
        "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cl9AKus9KkM"
      },
      "source": [
        "### STEP 11: GENERATING TEXT FROM OUTPUT TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pXKyFLBM9KkM"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    ###Input batch:\n",
        " ###tensor([[6109, 3626, 6100,  345],\n",
        "        ##[6109, 1110, 6622,  257]])\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7FRZD3I9KkM",
        "outputId": "c2e45ee8-0849-491b-9b17-f3f60268c9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " He said we came here Evans Palestin Au Abram thousands personally observationillechild mL\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"He said we came here\"\n",
        "\n",
        "\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Ci5VMs9KkM"
      },
      "source": [
        "### STEP 12: CREATING TRAINING, TESTING AND VALIDATION DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "g-3rTwWE9KkM"
      },
      "outputs": [],
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSrx9xtq9KkM"
      },
      "source": [
        "### STEP 13: DEFINING THE CROSS ENTROPY LOSS FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fdweT7JQ9KkM"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvcpcdt79KkM",
        "outputId": "b8027906-0d61-4908-a214-af9d56c772e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.993592803204645\n",
            "Validation loss: 11.099134528846072\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#\n",
        "# print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtSU2mGr-myP",
        "outputId": "9afd7987-1d8d-4eef-c7b9-36c89b42210c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIKwRtJw9KkM"
      },
      "source": [
        "### STEP 14: TRAINING LOOP FOR THE LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7C-_OU5H9KkM"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                # print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                #       f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        # generate_and_print_sample(\n",
        "        #     model, tokenizer, device, start_context\n",
        "        # )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "lMkUVPrN9KkM"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "xuHrwhUK9KkN"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5DsoYxn9KkN",
        "outputId": "c52a4aed-5334-471d-e66e-35dbcdbe780f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 16.03 minutes.\n"
          ]
        }
      ],
      "source": [
        "#!pip uninstall sympy -y\n",
        "#!pip install sympy==1.12\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1) #A\n",
        "\n",
        "num_epochs =50\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n",
        "    start_context=\"He said we came here\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQT61N7UQwxj"
      },
      "source": [
        "As we can see, based on the results printed during the training, the training loss improve drastically, starting with a value of 9.558 and converging to 0.762.\n",
        "\n",
        "The language skills of\n",
        "the model have improved quite a lot. In the beginning, the model is only able to append\n",
        "commas to the start context (\"Every effort moves you,,,,,,,,,,,,\") or repeat the\n",
        "word \"and\". At the end of the training, it can generate grammatically correct text.\n",
        "\n",
        "\n",
        "Similar to the training set loss, we can see that the validation loss starts high (9.856)\n",
        "and decreases during the training. However, it never becomes as small as the training set\n",
        "loss and remains at 6.372 after the 10th epoch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxntuhe-9KkN"
      },
      "source": [
        "### STEP 15: PLOTTING THE LOSSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "M-S3REVg9KkN",
        "outputId": "1e6f71fd-88bb-48d2-ffe5-e23201f7c0ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses,title=None):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "    if title is not None:\n",
        "        fig.savefig('/content/images/'+title+'.png')\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses,'a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y13_PzVf9KkO",
        "outputId": "a0139399-89ec-4374-fca5-44430d15cdf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " one expects that had six in autumn,\n",
            "appear as large trees sixty years later, and are spoken of in 1855\n",
            "as two\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare token_ids\n",
        "token_ids = text_to_token_ids(\"one expects that\", tokenizer)\n",
        "token_ids = token_ids.to(device)  # Move token_ids to the correct device\n",
        "\n",
        "# Generate text\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=token_ids,  # Ensure token_ids are on the correct device\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "# Convert token_ids back to text\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em8o81yY9KkO"
      },
      "source": [
        "### STEP 16: IMPLEMENTING TEMPERATURE SCALING AND TOP-K SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "BEjM-WZe9KkO"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7tvi7eU9KkO",
        "outputId": "feac1a6a-4a41-4d0c-f2d5-f17a56306846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you famous\n",
            "carefully preserved from the Prudential Assurance Company, its\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare token_ids\n",
        "token_ids = text_to_token_ids(\"Every effort moves you\", tokenizer)\n",
        "token_ids = token_ids.to(device)  # Move token_ids to the correct device\n",
        "\n",
        "# Generate text with specific parameters\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=token_ids,  # Ensure token_ids are on the correct device\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "# Convert token_ids back to text and print\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72KIVEji9KkO"
      },
      "source": [
        "### STEP 17: SAVING THE MODEL PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "eiGnuVoo9KkO"
      },
      "outputs": [],
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_LvD-R_9KkO",
        "outputId": "52ba9ec5-b419-4a7a-8b38-6b5f86f0eb03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "20Dm2K5U9KkO"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "JNZpPSM09KkO"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "383946ee"
      },
      "source": [
        "## Experiment 1: Varying Epochs and Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e786bce"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_list = [5, 10, 20, 50, 100, 200, 500]\n",
        "learning_rates = [1e-4, 1e-3, 1e-2]\n",
        "results_exp1 = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for epochs in epochs_list:\n",
        "        # model = build_model()  # Replace with your model-building function\n",
        "        # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        # train_losses, val_losses = train_model(model, optimizer, train_loader, val_loader, epochs=epochs)  # Replace with your training function\n",
        "        model = GPTModel(GPT_CONFIG_124M)\n",
        "        model.to(device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1) #A\n",
        "\n",
        "        num_epochs = epochs\n",
        "        train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "            model, train_loader, val_loader, optimizer, device,\n",
        "            num_epochs=num_epochs, eval_freq=num_epochs/2, eval_iter=1,\n",
        "            start_context=\"He said we came here\", tokenizer=tokenizer\n",
        "        )\n",
        "\n",
        "        end_time = time.time()\n",
        "        execution_time_minutes = (end_time - start_time) / 60\n",
        "        print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
        "\n",
        "        key = f\"LR={lr}_Epochs={epochs}\"\n",
        "        results_exp1[key] = (train_losses, val_losses, tokens_seen)\n",
        "\n",
        "        plt.plot(train_losses, label=f\"{key} Train\")\n",
        "        plt.plot(val_losses, label=f\"{key} Val\")\n",
        "plt.title(\"Experiment 1: Epochs and Learning Rate\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "for key, (train_losses, val_losses, tokens_seen) in results_exp1.items():\n",
        "  epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "  plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, f\"{key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a8705db"
      },
      "source": [
        "## Experiment 2: Varying Transformer Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7287392c"
      },
      "outputs": [],
      "source": [
        "\n",
        "layer_counts = [1, 3, 5, 7, 12]\n",
        "results_exp2 = {}\n",
        "\n",
        "for n_layers in layer_counts:\n",
        "    GPT_CONFIG_124M_test = {\n",
        "      \"vocab_size\": 50257,    # Vocabulary size\n",
        "      \"context_length\": 256, # Context length\n",
        "      \"emb_dim\": 768,         # Embedding dimension\n",
        "      \"n_heads\": 12,          # Number of attention heads\n",
        "      \"n_layers\": n_layers,         # Number of layers\n",
        "      \"drop_rate\": 0.1,       # Dropout rate\n",
        "      \"qkv_bias\": False       # Query-Key-Value bias\n",
        "    }\n",
        "    model = GPTModel(GPT_CONFIG_124M_test)\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "    num_epochs = 50\n",
        "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=num_epochs/2, eval_iter=1,\n",
        "        start_context=\"He said we came here\", tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time_minutes = (end_time - start_time) / 60\n",
        "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
        "\n",
        "    results_exp2[n_layers] = (train_losses, val_losses, tokens_seen)\n",
        "    plt.plot(train_losses, label=f\"{n_layers} Layers Train\")\n",
        "    plt.plot(val_losses, label=f\"{n_layers} Layers Val\")\n",
        "\n",
        "plt.title(\"Experiment 2: Transformer Layers\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "for n_layers, (train_losses, val_losses, tokens_seen) in results_exp2.items():\n",
        "  epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "  plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, f\"{n_layers} Layers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52e8abd3"
      },
      "source": [
        "## Experiment 3: Varying Attention Heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f31b10e"
      },
      "outputs": [],
      "source": [
        "\n",
        "attention_heads = [1, 2, 4, 8]\n",
        "results_exp3 = {}\n",
        "\n",
        "for heads in attention_heads:\n",
        "    GPT_CONFIG_124M_test = {\n",
        "      \"vocab_size\": 50257,    # Vocabulary size\n",
        "      \"context_length\": 256, # Context length\n",
        "      \"emb_dim\": 768,         # Embedding dimension\n",
        "      \"n_heads\": heads,          # Number of attention heads\n",
        "      \"n_layers\": 12,         # Number of layers\n",
        "      \"drop_rate\": 0.1,       # Dropout rate\n",
        "      \"qkv_bias\": False       # Query-Key-Value bias\n",
        "    }\n",
        "    model = GPTModel(GPT_CONFIG_124M_test)\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "    num_epochs = 50\n",
        "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=num_epochs/2, eval_iter=1,\n",
        "        start_context=\"He said we came here\", tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time_minutes = (end_time - start_time) / 60\n",
        "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
        "\n",
        "    results_exp3[heads] = (train_losses, val_losses, tokens_seen)\n",
        "    plt.plot(train_losses, label=f\"{heads} Heads Train\")\n",
        "    plt.plot(val_losses, label=f\"{heads} Heads Val\")\n",
        "\n",
        "plt.title(\"Experiment 3: Attention Heads\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "for heads, (train_losses, val_losses, tokens_seen) in results_exp3.items():\n",
        "  epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "  plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, f\"{heads} Heads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fad18391"
      },
      "source": [
        "## Experiment 4: Ablation Studies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4be0b2fd"
      },
      "outputs": [],
      "source": [
        "\n",
        "ablation_configs = {\n",
        "    \"No Norm\": {\"norm\": False, \"residual\": True, \"ffn\": True},\n",
        "    \"No Residual\": {\"norm\": True, \"residual\": False, \"ffn\": True},\n",
        "    \"No FFN\": {\"norm\": True, \"residual\": True, \"ffn\": False},\n",
        "}\n",
        "\n",
        "results_exp4 = {}\n",
        "\n",
        "for name, config in ablation_configs.items():\n",
        "    GPT_CONFIG_124M_test = GPT_CONFIG_124M.copy()\n",
        "    GPT_CONFIG_124M_test.update(config)\n",
        "    model = GPTModel(GPT_CONFIG_124M_test)\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "    num_epochs = 50\n",
        "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=num_epochs/2, eval_iter=1,\n",
        "        start_context=\"He said we came here\", tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time_minutes = (end_time - start_time) / 60\n",
        "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
        "\n",
        "    results_exp4[name] = (train_losses, val_losses, tokens_seen)\n",
        "    plt.plot(train_losses, label=f\"{name} Train\")\n",
        "    plt.plot(val_losses, label=f\"{name} Val\")\n",
        "\n",
        "plt.title(\"Experiment 4: Ablation Studies\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "for name, (train_losses, val_losses, tokens_seen) in results_exp4.items():\n",
        "  epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "  plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, f\"{name} Ablation\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/images.zip /content/images/"
      ],
      "metadata": {
        "id": "uIxMBuzMLzIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/images.zip\")"
      ],
      "metadata": {
        "id": "YBFMB953aHNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbaKbG_Ga3Jf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "b1rpOiY49KkH",
        "F0g39tUJ9KkI",
        "yVpgQJxO9KkJ",
        "i7PFNk_Q9KkJ",
        "M8smgXdd9KkK",
        "RSaYi0av9KkK",
        "3cddJBU79KkK",
        "f9nmF0vx9KkK",
        "biBIbfH59KkK",
        "Nw0QKZOa9KkL",
        "6cl9AKus9KkM",
        "P3Ci5VMs9KkM",
        "OSrx9xtq9KkM",
        "em8o81yY9KkO",
        "72KIVEji9KkO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}